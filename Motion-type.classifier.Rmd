---
title: "Motion-type classifier"
author: "Uran Chu"
date: "March 1, 2016"
output: html_document
---

This exercises import a fixed training and testing set of data to classify 5 different types of motions according to 54 covariates.

We will use library caret to do the data mining.

```{r libraries loader}
library(caret)
```

We first save the training set data, called pml-training.csv, and the test set data, called pml-testing.csv, to our home directory C://Rtools.  We then opened both in Excel, and got rid of all the columns with empty cells and NA's.  We discover that there is no missing data to impute because each variable has either all missing data in it, or all data.  We simply get rid of all the variables with either no data or NA listed under the column for both the training and testing sets.  They are the same variables and no further adjustments are needed.

First, we import the training and testing data sets into our home directory (mine is C://Rtools on my windows-base-operating-system-computer)  The training data set has 19,622 observations and 60 variables, while the testing data set has 20 test cases and 60 variables.


```{r dataImport}
training <- read.csv("C://Rtools//pml-traininga.csv")
testing <- read.csv("C://Rtools//pml-testinga.csv")
```

We then check out what variables are in the training set of data.

```{r variables}
str(training)
```

We discover that columns 1 to 5 are date-stamp and index type variables that are not useful as predictors, and the "classe" variable is the response variable with five possible classes, A, B, C, D, and E, that are at the 60th column.

We check out whether there are near-zero variance variables to exclude; predictor variables with almost all the same value are not very good predictors, and we look to screen these variables out before training.  We looked at predictor variables 6 to 59 because first 5 columns are time-stamps and index identifiers, while the 60th variable is the response.

```{r variable-filter}
nzv(training[, 6:59])
```

As the routine returns 1, the 6th column variable has near zero variance.  str function says it is a variable called new_window; it is a factor variable with two variables.  I decided to look further into it:

```{r new_window examiner}
table(training$new_window)
```

I understand that with such lop-sided proportion between the yes's and no's, we might consider leaving the variable out of the predictor set.  But 406 yes's is still substantial, and I decided to leave the variable in.

###Model building and its reasons (why I made the choices I did):
It is time to train our model.  As random forests or boosting are two algorithms that consistently win prediction contests, our first choice to train our classification model is the random forests. This will take a little time but it is well worth the time. I will use as much information that is available to be to train the random forest, meaning that I would use all the predictor variables (not the identification and time-stamping variables), to train my random forest.

###Cross-validation:
I take advantage of the inherent bootstrapping routine that is used for model averaging inside the random forest routine.  Thus, there is no cross-validation per say, but averaging over all bootstrapped samples used during random forests training have a similar effect to cross-validation.

###Expected out of sample errors:
I expect to make not more than 20% out of sample errors.  Since many of the classifiers that were          shown in the course videos had out-of-sample accuracy around 80%, and I am using one of the best predictors.

###What I did to build the classifier:
I built it using just the following one line code taught in the course.  The
line with ModFitRF in it.

```{r Random-Forest-classifier trainer}
attach(training)
ModFitRF <- train(classe~., method = "rf", data = training[,6:59])
detach(training)
```

After waiting a few hours, the model returns and we can use it for prediction on our test set:
     
```{r Random-Forest-classifier tester (validation)}
predict(ModFitRF, testing)
```
###Actual out of sample errors

The prediction across the 20 test cases is perfect, according to the Quiz 2 module during 4th week.  I received a 100% on prediction accuracy from that quiz.

Random forests, though it takes a long time to train, seems to be a great predictor.  